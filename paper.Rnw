\SweaveOpts{echo = FALSE, message = FALSE, warning = FALSE, fig.align = 'center', tidy = FALSE, comment = NA, cache = TRUE}

<<loadpackages, results="hide", echo=FALSE, cache=FALSE>>=
library(ggplot2)
library(Hmisc)
library(reshape2)
library(ADNIMERGE)
library(plyr)
options(digits=3)
theme_set(theme_bw())
@

\documentclass[twocolumn]{article}
\usepackage[margin=1in]{geometry}
\usepackage{algorithm, color, algpseudocode, amsmath, url, geometry, ctable }
\usepackage[square,numbers]{natbib}

%draft mode
\usepackage[colorlinks]{hyperref}
\usepackage[colorinlistoftodos, textwidth=4cm, shadow]{todonotes}
\usepackage[displaymath, tightpage]{preview}

\setlength{\marginparwidth}{1.2in}

%\newcommand{\marginnote}[1]{\-\marginpar[\raggedleft\footnotesize #1]{\raggedright\footnotesize #1}}
%\newcommand{\comment}[1]{\begin{kframe}{\textcolor{red}{#1}}\end{kframe}}
%\newcommand{\todo}[1]{\comment{TODO #1}}
%\newcommand{\mc}[1]{\comment{MC: #1}}
\begin{document}

\title{Hippocampal segmentation with MAGeT}
\author{Jon Pipitone}
\maketitle

\begin{abstract}
Neuroimaging research often relies on automated anatomical segmentations of MR
images of the brain. Current multi-atlas based approaches provide accurate
segmentations of brain images by propagating manually derived segmentations of
specific neuroanatomical structures to unlabelled data. These approaches often
rely on a large number of such manually segmented atlases that take significant
time and expertise to produce. We present an algorithm for the automatic
segmentation of the hippocampus that minimizes the number of atlases needed
while still achieving similar accuracy to multi-atlas approaches.
\todo[inline]{Finish this... }
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                               INTRODUCTION
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

The hippocampus is of particular interest to many researchers because it is
implicated in forms of brain dysfunction such as Alzheimer's disease and
schizophrenia, and has functional significance in cognitive processes such as
learning and memory.  For many research questions involving magnetic resonance
imaging (MRI) data accurate identification of the hippocampus and its
subregions is a necessary first step to better understand the individual
neuroanatomy of subjects.  

Currently, the gold standard for neuroanatomical segmentation is manual
delineation by an expert human rater.  This is problematic for segmentation of
the hippocampus for several reasons.  First, manual segmentation takes a
significant investment of time and expertise \citep{Hammers2003} which may not
be readily available to researchers or clinicians.  Second, the amount of data
produced in neuroimaging experiments increasingly exceeds the capacity for
identification of specific neuroanatomical structures by an expert manual
rater.  Third, the true definition of hippocampal anatomy in MR images is
disputed \citep{Geuze2004}, as evidenced by efforts to create an unified
segmentation protocol \citep{Jack2011}.  

Compounding each of these problems is the significant neuroanatomical
variability in the hippocampus throughout the course of aging, development, and
neuropsychiatric disorders \citep{Mouiha2011}.  Additionally, it may be
necessary to use several different hippocampal definitions or, in fact, make
specific modifications in the course of research. For example, Poppenk et al.
\citep{Poppenk2011} found that subdividing the hippocampus into anterior and
posterior regions resulted in a predictive relationship between volume
difference of those regions and recollection memory performance.  Thus, while
manual segmentation of the hippocampus is a necessary technique, to researchers
or clinicians who do not have access to the needed human expertise its use may
be infeasible.  

Automated segmentation techniques overcome the need for human expertise by
performing segmentations computationally.  A popular class of automated
methods, {\it multi-atlas-based segmentation}, rely on a set of expertly
labeled neuroanatomical atlases. Each atlas is warped to fit a subject's
neuroanatomy using nonlinear registration
techniques \citep{Collins1995,Klein2009}.  Atlas labels are then transformed by
this warping and a {\it label fusion} technique, such as voxel-wise voting, is
used to merge the competing label definitions from each atlas into a final
segmentation for a subject.  

Many descriptions of multi-atlas-based segmentation algorithms report relying
on an atlas library containing anywhere between 30 and 80 expertly labeled
brains \citep{Heckemann2011,Collins2010,Aljabar2009,Leung2010,Lotjonen2010}.  As
noted, the production of an atlas library requires significant manual effort,
and is limited since the choice of atlases or segmentation protocol may not
reflect the underlying neuroanatomical variability of the population under
study or be suited to answer the research questions at hand.

In this paper we propose an automated segmentation method to address the above
issues of existing multi-atlas-based methods. Principally, our method aims to
dramatically reduce the number of manually labelled atlases needed (under 10).
This is achieved by using the small atlas library to boot-strap a much larger
"template library", which is then used to segment the subjects in a similar
fashion to basic multi-atlas segmentation. This approach has the additional
advantage of using the unique subject population on hand to initialize the
segmentation process and improve accuracy.

The essential insight of generating a template library is not new.  Heckemann
\citep{Heckemann2006} compared generating a template library from a single atlas
to standard multi-atlas segmentation and found poor performance and so deemed
the approach as inviable.  The LEAP algorithm \citep{Wolz2010} proceeds by
iteratively segmenting the unlabelled image most similar to the atlas library
images and then incorporating the now-labelled image into the atlas library,
but requires 30 starting atlases.  The novelty of our method is to demonstrate
the possibility of producing comparable segmentation accuracy to these and
other multi-atlas-based methods while using significantly fewer manually
created atlases.

In our previous work \citep{Chakravarty2012}, we applied MAGeT brain to
segmentation of the human striatum, globus pallidus, and thalamus using a
single histologically-derived atlas. The main contribution of this paper is to
extend our approach to the human hippocampus and perform a thorough validation
over a range of atlas and template library sizes, which was not done in our
previous work.  Due to the small number of atlases required, our method can
easily accommodate different hippocampal definitions. Our aim is not to improve
on segmentation accuracy beyond existing methods, but instead to provide a
method that trades off manual segmentation expertise for computational
processing time while providing sufficient accuracy for clinical and research
applications.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                               METHODS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methods}


%%%%%                     MAGeT Brain Algorithm                            %%%%%

\subsection{MAGeT Brain Algorithm}
\todo{Make sure we explain what MAGeT stands for}
\todo{reference Chakravarty2012}

In this paper, we use the term {\it atlas} to mean any manually segmented MR
image, and the term {\it atlas library} to mean a set of such images.  We use
the term {\it template} to refer to any MR image, and associated labelling,
used to segment another image, and the term {\it template library} to refer to
a set of such images.  An atlas library may be used as a template library but,
as we will discuss, a template library may also be composed of images with
computer generated labellings. 

The simplest form of multi-atlas segmentation combines labellings derived from
several atlases by way of label fusion\citep{Collins2010}.  We will refer to
this as {\em basic multi-atlas segmentation}.  The primary steps are as
follows.  An atlas library and unlabelled MR images are given as input.  Each
atlas image is nonlinearly registered to each unlabelled image, and then each
atlas' labels are propagated via the resulting transformations.  The resulting
labels are fused to produce a single, definitive segmentation by some label
fusion method (for exampl, by voxel-wise majority voting). 

The segmentation approach we propose is best understood as an extension of
MAGeT brain adds a preliminary stage to this process in which the template
library is constructed rather than given as input.  As before, MAGeT brain
accepts an atlas library and unlabelled MR images as input.  Images for the
template library are selected from 

To create the template library, labels from each atlas image are propagated to each template
library image via the transformation resulting from a non-linear registration
between pair of images.  As a result, each template library image has a label
from each atlas.  Basic multi-atlas segmentation is then used to produce
segmentations for the entire set of unlabelled images (including those images
used in the template library). 

\todo{Note that our algorithm does not specify a registration algorithm or
fusion method\citep{Chakravarty2012}.  Performance on options is tested in our
experiments}

Source code can be found at \url{http://github.com/pipitone/MAGeTbrain}.

\begin{algorithm}
  \scriptsize
  \caption{Pseudocode for the MAGeT Brain algorithm}
  \label{pseudocodesdf}
  \begin{algorithmic}
    \Function{BasicMultiAtlasSegmentation}{Templates, Subjects}
      \ForAll{$subject$}
        \ForAll{$template$}
          \State propagate all labels for template to subject space
          \State store subject labels
        \EndFor
        \State fuse subject labels
      \EndFor
    \EndFunction
    \\
    \Function {MAGeTBrain}{Subjects, Atlases, n}
      \For{$i = 1 \to n$}
        \State choose a subject to be used as a template
        \State propagate labels from each atlas to template space
        \State store the template with all of its labels
      \EndFor
      \State MultiAtlas(Templates, Subjects)
    \EndFunction
  \end{algorithmic}
\end{algorithm}


\begin{figure}[h]
  \centering
    \includegraphics[width=0.5\textwidth]{figure/MAGeT-figure.png}
  \caption{Diagram of the MAGeT Brain algorithm}
\end{figure}

%%%%%                             Subjects                                 %%%%%

%  Describe subject datasets in general 
\subsection{Subjects}

\subsubsection{ADNI-1 1.5T Screening}
\todo{include image characteristics}
\todo{describe SNT manual segmentations}

<<ADNI-1-screening-demographics, echo=F, results="asis",cache=FALSE>>=
tab <- summary(DX.bl ~ AGE + PTGENDER + PTEDUCAT + PTETHCAT + CDRSB + ADAS13 + MMSE, 
               data = adnimerge, subset = VISCODE == 'bl',   
               method = "reverse", test=FALSE, overall=TRUE)

latex(tab, file="", size="scriptsize",  caption="ADNI-1 1.5T Screening demographics")
@

\todo{Check demographics table only includes 1.5T from the Screening}

\subsubsection{SZ First Episode Patients}
\todo{include image characteristics}

<<FEP-SZ-demographics, echo=F, results="asis",cache=FALSE>>=
fep_sz_demographics = read.csv("data/SZ_demographics.csv", na.strings=c("","."))
fep_sz_demographics = subset(fep_sz_demographics, Anon <= 81)
tab <- summary(DX ~ Age + Gender + Handedness + Education + SES + FSIQ,
               data=fep_sz_demographics, method = "reverse", test = FALSE)
latex(tab, file="", size="scriptsize", ctable=FALSE,
      caption="Schizophrenia First Episode Patient Demographics")
@


%%%%%                       Image pre-processing                           %%%%%

\subsection{Image pre-processing}

Before images were registered, the N3 algorithm [16] is first used to minimize
the intensity nonuniformity in each of the atlases and unlabeled subject
images.   


%%%%%                       Registration Methods                           %%%%%

\subsection{Registration}

\subsubsection{Automatic Normalization and Image Matching and Anatomical Labeling (ANIMAL)}
The ANIMAL algorithm carries out Image registration is in two phases. In the
first, a 12-parameter linear transformation (3 translations, rotations, scales,
shears) is estimated between images using an algorithm that maximizes the
correlation between blurred MR intensities and gradient magnitude over the
whole brain \citep{Collins}. In the second phase, nonlinear registration is completed using
the ANIMAL algorithm \citep{Collins1995}: an iterative procedure that estimates a 3D
deformation field between two MR images. At first, large deformations are
estimated using blurred version of the input data. These larger deformations
are then input to subsequent steps where the fit is refined by estimating
smaller deformations on data blurred with a Gaussian kernel with a smaller
FWHM. The final transformation is a set of local translations defined on a bed
of equally spaced nodes that were estimated through the optimization of the
correlation coefficient. 

For the purposes of this work we used the regularization parameters optimized 
in Robbins et al. \citep{Robbins2004}. 

\todo{link to MNI website here}
\todo{show command line used}

\subsubsection{Automatic Normalization Tools (ANTS)}

ANTs is a diffeomorphic registration algorithm which provides great flexibility
over the choice of transformation model, objective function, and the
consistency of the final transformation. The transformation is estimated in a
hierarchical fashion where the MRI data is subsampled, allowing large
deformations to be estimated and successively refined at later hierarchical
stages (where the data is subsampled to a finer grid). The deformation field
and the objective function are regularized with a Gaussian kernel at each level
of the hierarchy. The ANTs algorithm is freely available
\url{http://www.picsl.upenn.edu/ANTS/}. We used an implementation of the ANTS
algorithm compatible with the MINC data format, mincANTS
\url{https://github.com/vfonov/mincANTS}.

We used the following command line when running the ANTS command, 
{\small
\begin{verbatim}
  mincANTS 3 -m PR[target_file.mnc,source_file.mnc,1,4] 
   --number-of-affine-iterations 10000x10000x10000x10000x10000 
   --affine-gradient-descent-option 0.5x0.95x1.e-4x1.e-4
   --use-Histogram-Matching --MI-option 32x16000
   -r Gauss[3,0] -t SyN[0.5] -i 100x100x100x20
   -o transformation.xfm
 \end{verbatim}
}
These settings were adapted from the "reasonable starting point" given in the
ANTS manual. \todo{include a reference to manual?}


%%%%%                           Label Fusion                               %%%%%

\subsection{Label Fusion}

Label fusion is term given to the process of combining the information from
several candidate labellings for an MR image into a single labelling.  In this
paper we explore the benefits of three different fusion methods. 

\subsubsection{Voxel-wise Majority Vote}

Labels are propagated from all template library images to a subject.  Each
output voxel is given the most frequent label at that voxel location amongst
all candidate labellings.  Ties are broken arbitrarily.

\subsubsection{Cross-correlation Weighted Majority Vote}

An optimal combination of subjects from the template library has previously been
shown to improve segmentation accuracy \citep{Aljabar2009,Collins2010}.  In this
method, each template library image is ranked in similarity to each unlabelled 
image by the normalized cross-correlation (CC) of image intensities after linear
registration, over a region of interest (ROI) generously encompassing the 
hippocampus.  Only the top ranked
template library image labels are used in a voxel-wise majority vote. The ROI
is heuristically defined as the extent of all atlas labels after linear
registration to the template, dilated by three voxels \citep{Chakravarty2012}.  
The number of top ranked template library image labels is a configurable parameter.

The {\tt xcorr\_vol utility} from the ANIMAL toolkit is used to calculate the 
cross-correlation similarity measure.  
 
\subsubsection{Normalised Mutual Information Weighted Majority Vote}
 
This method is similar to cross-correlation weighted voting 
except that image similarity is calculated by the normalised 
mutual information score over the region of interest. 
\todo{is there a reference for NMI voting?}
 
The {\tt itk\_similarity} utility from the EZMinc toolkit is used to calculate 
the normalised mutual information measure.

%%%%%                        Goodness-of-fit                               %%%%%

\subsection{Goodness-of-fit}

Two segmentation can be compared using the Dice Kappa ($\kappa$) overlap metric:

\begin{equation*} 
\kappa=\frac{2a}{2a+b+c}
\end{equation*}

where $a$ is the number of voxels common to both segmentations and $b+c$ is the
sum of the voxels uniquely identified by either segmentation.  Using this metric
we can compare an automatically generated segmentation to a gold-standard
segmentation.

%%%%%
%%%%%                            Experiments                               %%%%%
%%%%%
\subsection{Experiments}

Experiments were performed to assess the performance of MAGeT brain with various 
parameter settings as well as on diverse datasets.  In each experiment we contrast 
the performance of MAGeT brain with that standard single- and multi-atlas 
segmentations derived from the same atlas library. 

%%%%%
%%%%%                 Experiments: ADNI-1 cross-validationn               %%%%%
%%%%%
\subsubsection{ADNI-1 cross-validation}

To test the accuracy of the MAGeT brain algorithm with different parameter 
settings, repeated random sub-sampling cross-validation (RRSCV) was performed
on a subset of the ADNI-1 dataset.

{\bf Dataset evaluated.} 69 1.5T images were randomly selected from the 
{\em ADNI1:Screening 1.5T} standardized dataset.  Demographics for this subset 
are shown in Table X
\todo{FIXME: table reference}. 

<<ADNI-69-demographics, echo=F, results="asis",cache=FALSE>>=
adni_69_RIDs = c("295","413","619","685","729","782","938","954",
                 "1018","1155","907","981","222","324","448","546",
                 "553","572","602","610","814","1341","675","681",
                 "731","1130","41","68","70","101","249","293","316",
                 "414","698","1206","1222","1339","751","1030","3",
                 "5","10","16","22","53","168","183","241","1205",
                 "328","1095","991","213","159","337","343","642",
                 "753","1109","1217","29","56","1188","1293","149",
                 "382","431","1202")
tab <- summary(DX.bl ~ AGE + PTGENDER + PTEDUCAT + PTETHCAT + CDRSB + ADAS13 + MMSE, 
               data = adnimerge, subset = VISCODE == 'bl' & RID %in% adni_69_RIDs, 
               method = "reverse", test=FALSE, overall=TRUE)

latex(tab, file="", size="scriptsize",landscape=FALSE, ctable=FALSE,
      caption="ADNI-1 cross-validation subset demographics")
@

{\bf Atlas and template libary.}  Atlases consisted of images taken from the
dataset, with corresponding manual labels provided by SNT.  Atlas library size
was varied from 3 to 9 images.  The remaining images were segmented, with the
template library size varying from 3 to 20 images.  Template library images were
selected randomly from the images to be segmented. 

{\bf Registration method.}  Both the ANTS and ANIMAL registration methods were
used.

\todo{In ANIMAL, images aligned to TAL space}

{\bf Label fusion.} Majority vote, Cross-correlation weighted majority vote, and
Normalized Mutual Information weighted majority vote are used.  With the 
weighted majority vote fusion methods, the number of top labels used in the 
fusion was varied from 3 to 20 images.

{\bf Evaluation.}  Repeated random sub-sampling cross-validation (RRSCV)
consists of repeated trials in which items from the dataset are randomly
assigned to a training set or validation set. In each trial, performance on the
validation set is measured, and then averaged across all trials. 

We performed RRSCV on each combination of parameters listed above: atlas library
size, template library size, registration method, and label fusion method.  We
performed 10 trials per parameter combination.  In each validation trial, the
training set consisted of the images used as atlases, and the validation set
consisted of the images to be segmented. The MAGeT brain algorithm and the basic
multi-atlas segmentation procedure were applied to segment the images in the
validation set.  Additionally, in each trial, the single-atlas segmentation was
obtained for each atlas-template.  

The gold-standard for the segmentation accuracy of images in the validation set
was the SNT manual labels.

\todo{ show off the number of registrations/comparisions we did}


%%%%%
%%%%%               Experiments: ADNI-1 Screening Validation               %%%%%
%%%%%
\subsubsection{ADNI-1 Screening Validation}

To test the accuracy of MAGeT brain on a real-world task we segment the entire
ADNI-1 dataset using an atlas set that is not representative of the subject set.

{\bf Dataset evaluated.} All images from the {\em ADNI1:Screening 1.5T} 
standardized dataset.

{\bf Atlas and template libary.} The atlas library consisted of the entire 
Winterburn atlas set.  The Winterburn atlases are digital segmentations of the
hippocampus in five in-vivo 300u isotropic T1-weighted MR scans, and include 
subfield segmentations for the cornus ammonis (CA) 1, CA4, dentate gyrus, 
subiculum, and CA 2 and 3 combined. Subjects in the Winterburn atlases range in 
age from 29-57 years (mean age of 37), and include two males and three females.  

The template library consisted of 21 randomly selected images from the ADNI1 
data dataset (7 healthy, MCI and AD subjects).  

{\bf Registration method.} ANTS, as it performed best in the cross-validation
experiment. 

\todo{Is it okay to reference results whilst still in Methods?}

{\bf Label fusion.} Majority vote, as it is simpliest to run and performed 
equally well in cross-validation experiment.

{\bf Evaluation.}

Since hippocampal segmentation protocols differ between the ADNI labels and
Winterburn atlases, this poses a problem for direct similarity comparisons
between labels produced by MAGeT brain and the ADNI labels.  

\todo{explain why we did(n't) resegment the ADNI images with the low-res
protocol and compare directly like that.} 

To evaluate the performance of MAGeT brain, we correlate our segmentation
volumes with manual segmentation volumes, as well as with hippocampal volumes
of established automated segmentation methods.

Additionally, we compared classification accuracy of subjects
by diagnosis based on hippocampal volume using both the SMT labels and our
produced labels.  

\todo{Include a description of validation process.. Note that we could use rms
{\tt validate} or t-test: contrast with QDA or LDA (Coupe 2011) used in LOOCV,
with adni/our segmentations}


%%%%%
%%%%%          Experiments: SZ First Episode Patient Validation           %%%%%
%%%%%
\subsubsection{SZ First Episode Patient Validation}

{\bf Dataset evaluated.} To validate that MAGeT performance generalises to other
diseases, we measure the performance using the best parameter settings previous
found, on a dataset consiting of first episode schizophrenia patients.
\todo{Refer to the FEP description above... or just include it here}.
 
{\bf Atlas and template libary.} - two different atlas sets: a manual
hippocampal segmentation of patients, and Winterburn atlas set.  

{\bf Registration method.} ANTS.

{\bf Label fusion.} Majority vote.

{\bf Evaluation.} We validate the FEP-atlas segmentations using Dice's Kappa,
and the Winterburn-atlas segmentations by correlating volumes. 


%%%%%
%%%%%              Experiments: Winterburn Atlases Validation              %%%%%
%%%%%
\subsubsection{Winterburn Atlases Validation}
{\bf Dataset evaluated.} - T1 BRAVO scans of the same subjects included in the 
Winterburn atlas set.  These scans are taken within .... weeks of the scans for 
the Winterburn atlases.

{\bf Atlas and template libary.} - Atlas library is Winterburn T1 atlases.  
Template library consists of all five T1 BRAVOs, plus 15 T1 healthy control images.

{\bf Registration method.} ANTS

{\bf Label fusion.} Majority vote.

{\bf Evaluation.}  - Leave one out cross-validation (LOOCV) in which all five
subjects are segmented in separate runs of MAGeT brain.  In each run, the subject
to be segmented is excluded from the Atlas library (so only four atlases are used).

Segmentation accuracy is judged by difference in hippocampal volume.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                               Results
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}

%%%%%
%%%%%                  Results: ADNI-1 cross-validationn                   %%%%%
%%%%%
\subsection{ADNI-1 Cross-Validation}

- find significant improvement over multi-atlas performed with the same
  parameters. Also, find smoothed performance is monotonically increasing but
  asymptotic in size of both template and atlas library, with peak performance
  reached after 15 templates. 
  
\todo{Can we statistically capture what "peak" performance is?  Something like,
the point at which gains become statistically insignificant?}

\begin{figure}
<<ADNI1_cross_validation_means, cache=TRUE, fig=TRUE>>=
t_data       = read.csv(file='data/a2a_tracc_results_2012_08_30.csv')
a_data       = read.csv(file='data/a2a_ants_results_2012_08_30.csv')

t_multiatlas = read.csv(file='data/a2a_tracc_multiatlas_2012_08_14.csv')
a_multiatlas = read.csv(file='data/a2a_ants_multiatlas_2012_08_14.csv')
t_ma_means   = aggregate( k ~ num_atlases , data=t_multiatlas, mean)
a_ma_means   = aggregate( k ~ num_atlases , data=a_multiatlas, mean)

# add registration method to data frames (before joining)
t_data       = cbind(t_data, reg_method = "ANIMAL")
a_data       = cbind(a_data, reg_method = "ANTS")

# add column with kappa - multiatlas mean
# num_atlases - 2 is a cheap trick to perform the necessary lookup into the ma_means tables
t_data$k_minus_ma = t_data$k - t_ma_means[ t_data$num_atlases - 2, 2 ]
a_data$k_minus_ma = a_data$k - a_ma_means[ a_data$num_atlases - 2, 2 ]

all_data     = rbind(t_data,a_data)

# use top_n as the number of templates for weighted voting schemes
all_data[ all_data$method != "majvote", ]$num_templates = all_data[ all_data$method != "majvote", ]$top_n

t_naive      = read.csv(file='data/a2a_tracc_naive.csv')
a_naive      = read.csv(file='data/a2a_ants_naive_2012_08_30.csv')
t_mean_naive = mean(t_naive$k)
a_mean_naive = mean(a_naive$k)
df_naive     = data.frame(
                value = c(rep(t_mean_naive,3),rep(a_mean_naive,3)),
                method = rep(c("majvote", "nmi", "xcorr"),2), 
                reg_method = c(rep("ANIMAL",3),rep("ANTS",3)))

method_labels <- list("Majority Vote" = "majvote", "NMI Vote" = "nmi", "Cross-correlation Vote" = "xcorr")
levels(all_data$method) <- method_labels
levels(df_naive$method) <- method_labels

ggplot(all_data, aes(x=num_templates, y=k, colour=as.factor(num_atlases))) + 
  facet_grid(method ~ reg_method) + 
  geom_smooth() + 
  geom_hline(data = df_naive, aes(yintercept = value, linetype="dashed")) + 
  scale_colour_hue(name="Atlases") +
  xlab( "Number of Templates" ) + ylab( "Mean Kappa" ) 
@
  \caption{Comparison of MAGeT performance on ADNI-1 subset.  Smoothing line
  fitted using GAM (generalised additive model) from R with defaults from
  ggplot2 (formula: y ~ s(x, bs = "cs"))}
  \label{ADNI1_cross_validation_means}
\end{figure}

\begin{figure}
<<ADN1-cross-validation-MA-difference, cache=TRUE, fig=TRUE>>=
ggplot(all_data, aes(x=num_templates, y=k_minus_ma, colour=as.factor(num_atlases))) + 
  facet_grid(method ~ reg_method) + 
  geom_smooth() + 
  scale_colour_hue(name="Atlases") +
  xlab( "Number of Templates" ) + 
  ylab( "Difference in mean Kappa between voting method and multiatlas" )
@
  \caption{Difference in mean Kappa between MAGeT brain and multi-atlas}
  \label{ADNI1_cross_validation_MA_difference}
\end{figure}



<<a2a_multiatlas_means, results="asis",cache=FALSE>>=
latex(merge(a_ma_means, t_ma_means, by="num_atlases"),
colheads=c("Atlases", "ANTS", "ANIMAL"), file="", dec=2,
	caption="Multi-atlas means")
@

%\begin{table}
%    \begin{tabular}{c|c|c}
%        Number of Atlases & ANIMAL Kappa (Jaccard) & ANTS Kappa (Jaccard) \\ \hline
%        3                 & 0.76 (0.63)             & 0.80 (0.69)          \\ 
%        4                 & 0.75 (0.62)             & 0.79 (0.67)          \\ 
%        5                 & 0.79 (0.66)             & 0.82 (0.71)          \\ 
%        6                 & 0.78 (0.65)             & 0.82 (0.70)          \\ 
%        7                 & 0.80 (0.67)             & 0.83 (0.72)          \\ 
%        8                 & 0.79 (0.67)             & 0.83 (0.72)          \\ 
%        9                 & 0.80 (0.68)             & 0.84 (0.73)          \\
%    \end{tabular}
%\end{table}

Ideas:
\begin{itemize}
\item more atlases -> better performance
\item  larger template library -> better performance, but tails off around 10-15 templates
\item  no significant difference between majority or weighted vote methods (haven???t tested this statistically though). 
\item  consistently performs better than average naive performance by XXX
\item  using ANTS, with a large enough template library (>12) MAGeT brain performs
  better than the average multi-atlas approach with the same number of atlases.
  using ANIMAL, 5 or more atlases needed before boost seen. 
\item  more atlases -> smaller template library required to improve on average multi-atlas performance
\item  discuss variance?  best/worst case? --how often do we expect random template library selection to work decently
\end{itemize}

\todo{show cost (in registrations) / benefit tradec off graph:  show number of
registrations per Kappa?  or hours of manual labour per Kappa?)}

%%%%%
%%%%%                  Results: ADNI-1 Screening Validation               %%%%%
%%%%%
\subsection{ADNI-1 Screen Validation}


Ideas: 
\begin{itemize}
\item  A2A shows that if atlas population strongly(?) represents subject set
variability, then free choice from atlas population will produce improvements
(we know this b/c of extensive validation trials). 
\item  what about in the case where atlas population doesn???t strongly
represent subject set variability (e.g. a priori atlas set)?  then, we can use
atlas selection to refine atlas set? 
\end{itemize}

\todo{Kappa against our manual rater is low... explain that}

<<ADNI-baseline-volumes-prep,echo=F,cache=T>>=
yr1   = read.csv("data/ADNI_baseline_volumes/ADNI1_Complete_1Yr_1.5T_11_15_2012.csv")
mb    = read.csv("data/ADNI_baseline_volumes/ADNI1_1.5T_MAGeT_volumes.csv")
maper = read.csv("data/ADNI_baseline_volumes/MAPER_volumes.csv")
snt   = read.csv("data/ADNI_baseline_volumes/UCSFSNTVOL.csv")
ucsd  = read.csv("data/ADNI_baseline_volumes/UCSDVOL.csv")
fs    = read.csv("data/ADNI_baseline_volumes/UCSFFSX_02_15_12.csv")
fsl   = read.csv("data/ADNI_baseline_volumes/ADNI1_1.5T_FSLFIRST_volumes.csv")

# recode VISCODE
viscode_levels = list("bl"=1,"bl"=2, "m06"=3,"m12"=4)
yr1$VISCODE <- as.factor(yr1$Visit)
levels(yr1$VISCODE) <-viscode_levels

yr1$Scaled_2 <- grepl("Scaled_2", yr1$Description)

subjects = subset(yr1, VISCODE == 'bl' & Scaled_2 == FALSE)

# fetch just the columns we need, and do a little renaming
mb_pruned     = data.frame(MAGeT_L = mb$Left_Hippo, MAGeT_R = mb$Right_Hippo, Source = mb$SourceImageID)
maper_pruned  = data.frame(MAPER_L = maper$Left_Hippo, MAPER_R = maper$Right_Hippo, Source = maper$SourceImageID)
snt_pruned    = data.frame(SNT_L = snt$LEFTHIPPO, SNT_R = snt$RIGHTHIPPO, Source = snt$IMAGEUID)
ucsd_pruned   = data.frame(RID=ucsd$RID, VISCODE=ucsd$VISCODE, UCSD_L=ucsd$LHIPPOC, UCSD_R=ucsd$RHIPPOC)
fs_pruned     = subset(fs, TEMPQC != "Fail")
fs_pruned     = data.frame(FS_L=fs_pruned$ST29SV, FS_R=fs_pruned$ST88SV, Source = fs_pruned$IMAGEUID)
fsl_pruned    = data.frame(FSL_L=fsl$X17, FSL_R=fsl$X53, Source=fsl$Source)

# diagnoses
dx = adnimerge[c("DX.bl", "RID", "VISCODE")]

# Now create one data.frame with all the yr1 volume data we have from each datasource
combined = merge(subjects, mb_pruned   , by.x="Image.Data.ID", by.y="Source", all.x=TRUE)
combined = merge(combined, maper_pruned, by.x="Image.Data.ID", by.y="Source", all.x=TRUE)  # only Baseline
combined = merge(combined, snt_pruned  , by.x="Image.Data.ID", by.y="Source", all.x=TRUE)
combined = merge(combined, ucsd_pruned , by=c("RID", "VISCODE"), all.x =TRUE)              # TODO: this a unique key?
combined = merge(combined, fs_pruned   , by.x="Image.Data.ID", by.y="Source", all.x=TRUE)
combined = merge(combined, fsl_pruned  , by.x="Image.Data.ID", by.y="Source", all.x=TRUE)
combined = merge(combined, dx          , by=c("RID","VISCODE"), all.x=TRUE)

attach(combined)
vols = data.frame(RID = RID, VISCODE = VISCODE, DX = DX.bl,
                  MAGeT = MAGeT_L + MAGeT_R, 
                  MAPER = MAPER_L + MAPER_R, 
                  SNT   = SNT_L + SNT_R, 
                  UCSD  = UCSD_L + UCSD_R,
                  FS    = FS_L + FS_R, 
                  FSL   = FSL_L + FSL_R)
@

\begin{figure}[h]
<<ADNI-baseline-volumes-boxplot,echo=F,fig=T, fig.align='center',dependson='ADNI-baseline-volumes-prep',cache=T>>=
totals = vols
qplot(DX,value,data=melt(totals,measure.vars=c("FS", "FSL","UCSD", "MAPER","MAGeT","SNT"),na.rm=T),
      colour=variable,geom="boxplot") + 
        xlab("Diagnosis") + ylab("Total Hippocampal Volume (mm3)")

@
  \caption{Comparison of HC volumes by FreeSurfer (FSF), MAGeT brain (MAGeT), MAPER, and manual (SNT).}
  \label{ADNI-baseline-volumes-boxplot}
\end{figure}

\begin{figure}[h]
<<ADNI-baseline-volumes-plot,echo=F,fig=T, fig.align='center',dependson='ADNI-baseline-volumes-boxplot',cache=T>>=
by_snt_volumes = melt(totals, measure.vars=c("FSL","MAPER","MAGeT", "FS", "UCSD"))
fsl=subset(by_snt_volumes, variable=="FSL")
maper=subset(by_snt_volumes, variable=="MAPER")
maget=subset(by_snt_volumes, variable=="MAGeT")

qplot(SNT,value,data=by_snt_volumes,
      colour=variable, geom="smooth", method="lm") + 
    geom_point() +
#     geom_text(aes(
#       label=paste("FSL       R=", round(cor(fsl$SNT,fsl$value, use="complete.obs"), 2), 
#                   ", p<", round(as.double(t.test(fsl$SNT,fsl$value, paired=T)["p.value"]), 2)),
#       x=1800,y=10000, hjust=0,size=2)) + 
#     geom_text(aes(
#       label=paste("MAPER R=", round(cor(maper$SNT,maper$value, use="complete.obs"), 2), 
#                   ", p<", t.test(maper$SNT,maper$value, paired=T)["p.value"]), 
#       x=1800,y= 9500, hjust=0, size=2)) +
#     geom_text(aes(
#       label=paste("MAGeT  R=", round(cor(maget$SNT,maget$value, use="complete.obs"),2), 
#                   ", p<", t.test(maget$SNT,maget$value, paired=T)["p.value"]), 
#       x=1800,y= 9000, hjust=0, size=2)) +
    xlab("Total Hippocampal Volume (Manual Segmentation)") + 
    ylab("Total Hippocampal Volume (Automatic Segmentation)")
@
  \caption{{\bf ADNI Baseline cohort.} Comparison of HC volumes by FreeSurfer
  (FSF), MAGeT brain (MAGeT), MAPER, and manual (SNT).}
  \label{ADNI-baseline-volumes-plot}
\end{figure}

%%%%%
%%%%%             Results: First Episode Patient Validation          %%%%%
%%%%%
\subsection{First Episode Schizophrenic Patients}
High volume correlation between Winterburn segmentation volumes and ground
truth.  (High-ish?) Kappa when using manual segmentations as Atlases. 

\begin{figure}
<<SZ_volumes,echo=F,fig=T, fig.align='center',cache=T>>=
SZ_volumes=read.csv("data/SZ_volumes_by_method.csv")
qplot(Manual,MAGeT,data=SZ_volumes) + geom_smooth(method="lm") + 
#   geom_text(aes(
#       label=paste("R=", round(cor(Manual,MAGeT, use="complete.obs"), 2), 
#                   ", p<", t.test(Manual, MAGeT, paired=T)["p.value"]),
#       x=6500,y=10000, hjust=0,size=2)) +
    xlab("Total Hippocampal Volume (Manual Segmentation)") + 
    ylab("Total Hippocampal Volume (MAGeT Segmentation)")

@
  \caption{{\bf First Episode Schizophrenic Patients.} Comparison of total
  HC volumes for MAGeT against manually rated Hippocampal volumes}
  \label{SZ_volumes}
\end{figure}

%%%%%
%%%%%              Experiments: Winterburn Atlases Validation              %%%%%
%%%%%
\subsection{Winterburn Atlases Validation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                Discussion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                               Conclusion 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

\bibliographystyle{plain}
\bibliography{references}
\end{document}
